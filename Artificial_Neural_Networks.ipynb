{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Artificial_Neural_Networks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNZo4S35aKKUBo8yzEGpXNs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/18WH1A1244/WiseML/blob/main/Artificial_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qp3fwAZiaZL"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "Tgy_Bb_fjQmx",
        "outputId": "e5578526-8015-45cc-a22d-3329cabadb99"
      },
      "source": [
        "data = pd.read_csv('/content/Wine.csv')\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>14.23</th>\n",
              "      <th>1.71</th>\n",
              "      <th>2.43</th>\n",
              "      <th>15.6</th>\n",
              "      <th>127</th>\n",
              "      <th>2.8</th>\n",
              "      <th>3.06</th>\n",
              "      <th>.28</th>\n",
              "      <th>2.29</th>\n",
              "      <th>5.64</th>\n",
              "      <th>1.04</th>\n",
              "      <th>3.92</th>\n",
              "      <th>1065</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>14.20</td>\n",
              "      <td>1.76</td>\n",
              "      <td>2.45</td>\n",
              "      <td>15.2</td>\n",
              "      <td>112</td>\n",
              "      <td>3.27</td>\n",
              "      <td>3.39</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.97</td>\n",
              "      <td>6.75</td>\n",
              "      <td>1.05</td>\n",
              "      <td>2.85</td>\n",
              "      <td>1450</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   1  14.23  1.71  2.43  15.6  127  ...   .28  2.29  5.64  1.04  3.92  1065\n",
              "0  1  13.20  1.78  2.14  11.2  100  ...  0.26  1.28  4.38  1.05  3.40  1050\n",
              "1  1  13.16  2.36  2.67  18.6  101  ...  0.30  2.81  5.68  1.03  3.17  1185\n",
              "2  1  14.37  1.95  2.50  16.8  113  ...  0.24  2.18  7.80  0.86  3.45  1480\n",
              "3  1  13.24  2.59  2.87  21.0  118  ...  0.39  1.82  4.32  1.04  2.93   735\n",
              "4  1  14.20  1.76  2.45  15.2  112  ...  0.34  1.97  6.75  1.05  2.85  1450\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "lNSF2sgFjrXY",
        "outputId": "ba1f6ccc-6287-4a50-de99-c7d8e46cab7c"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df, train_df['1065'], test_df['1065'] = train_test_split(data.drop(['1065'], axis=1), data['1065'], test_size=0.2, random_state=42, shuffle=True)\n",
        "train_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>14.23</th>\n",
              "      <th>1.71</th>\n",
              "      <th>2.43</th>\n",
              "      <th>15.6</th>\n",
              "      <th>127</th>\n",
              "      <th>2.8</th>\n",
              "      <th>3.06</th>\n",
              "      <th>.28</th>\n",
              "      <th>2.29</th>\n",
              "      <th>5.64</th>\n",
              "      <th>1.04</th>\n",
              "      <th>3.92</th>\n",
              "      <th>1065</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>3</td>\n",
              "      <td>12.45</td>\n",
              "      <td>3.03</td>\n",
              "      <td>2.64</td>\n",
              "      <td>27.0</td>\n",
              "      <td>97</td>\n",
              "      <td>1.90</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.63</td>\n",
              "      <td>1.14</td>\n",
              "      <td>7.50</td>\n",
              "      <td>0.67</td>\n",
              "      <td>1.73</td>\n",
              "      <td>880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>3</td>\n",
              "      <td>12.53</td>\n",
              "      <td>5.51</td>\n",
              "      <td>2.64</td>\n",
              "      <td>25.0</td>\n",
              "      <td>96</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.63</td>\n",
              "      <td>1.10</td>\n",
              "      <td>5.00</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1.69</td>\n",
              "      <td>515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>2</td>\n",
              "      <td>12.29</td>\n",
              "      <td>3.17</td>\n",
              "      <td>2.21</td>\n",
              "      <td>18.0</td>\n",
              "      <td>88</td>\n",
              "      <td>2.85</td>\n",
              "      <td>2.99</td>\n",
              "      <td>0.45</td>\n",
              "      <td>2.81</td>\n",
              "      <td>2.30</td>\n",
              "      <td>1.42</td>\n",
              "      <td>2.83</td>\n",
              "      <td>406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>3</td>\n",
              "      <td>13.48</td>\n",
              "      <td>1.67</td>\n",
              "      <td>2.64</td>\n",
              "      <td>22.5</td>\n",
              "      <td>89</td>\n",
              "      <td>2.60</td>\n",
              "      <td>1.10</td>\n",
              "      <td>0.52</td>\n",
              "      <td>2.29</td>\n",
              "      <td>11.75</td>\n",
              "      <td>0.57</td>\n",
              "      <td>1.78</td>\n",
              "      <td>620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1</td>\n",
              "      <td>14.22</td>\n",
              "      <td>3.99</td>\n",
              "      <td>2.51</td>\n",
              "      <td>13.2</td>\n",
              "      <td>128</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.04</td>\n",
              "      <td>0.20</td>\n",
              "      <td>2.08</td>\n",
              "      <td>5.10</td>\n",
              "      <td>0.89</td>\n",
              "      <td>3.53</td>\n",
              "      <td>760</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     1  14.23  1.71  2.43  15.6  127  ...   .28  2.29   5.64  1.04  3.92  1065\n",
              "156  3  12.45  3.03  2.64  27.0   97  ...  0.63  1.14   7.50  0.67  1.73   880\n",
              "136  3  12.53  5.51  2.64  25.0   96  ...  0.63  1.10   5.00  0.82  1.69   515\n",
              "98   2  12.29  3.17  2.21  18.0   88  ...  0.45  2.81   2.30  1.42  2.83   406\n",
              "158  3  13.48  1.67  2.64  22.5   89  ...  0.52  2.29  11.75  0.57  1.78   620\n",
              "38   1  14.22  3.99  2.51  13.2  128  ...  0.20  2.08   5.10  0.89  3.53   760\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzEroh6AkIhN"
      },
      "source": [
        "predictors = train_df.columns[:-1]\n",
        "target = '1065'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87n76Htmkb9p",
        "outputId": "f13c1d84-5cbf-4567-bfff-228391381a14"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_train = scaler.fit_transform(train_df)\n",
        "scaled_train"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.30409357, 0.45256917, ..., 0.14285714, 0.16849817,\n",
              "        0.42938659],\n",
              "       [1.        , 0.32748538, 0.94268775, ..., 0.30769231, 0.15384615,\n",
              "        0.16904422],\n",
              "       [0.5       , 0.25730994, 0.48023715, ..., 0.96703297, 0.57142857,\n",
              "        0.09129815],\n",
              "       ...,\n",
              "       [0.        , 0.64912281, 0.21146245, ..., 0.81318681, 0.58974359,\n",
              "        0.7360913 ],\n",
              "       [0.5       , 0.25730994, 0.41304348, ..., 0.67032967, 0.74358974,\n",
              "        0.0085592 ],\n",
              "       [0.5       , 0.11988304, 0.19367589, ..., 0.43956044, 0.42857143,\n",
              "        0.09771755]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "Ig_ZtW_KkeVD",
        "outputId": "168e865e-063d-47e9-d70c-55aa8cc9f07d"
      },
      "source": [
        "scaled_train_df = pd.DataFrame(scaled_train, columns=train_df.columns.values)\n",
        "scaled_train_df"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>14.23</th>\n",
              "      <th>1.71</th>\n",
              "      <th>2.43</th>\n",
              "      <th>15.6</th>\n",
              "      <th>127</th>\n",
              "      <th>2.8</th>\n",
              "      <th>3.06</th>\n",
              "      <th>.28</th>\n",
              "      <th>2.29</th>\n",
              "      <th>5.64</th>\n",
              "      <th>1.04</th>\n",
              "      <th>3.92</th>\n",
              "      <th>1065</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.304094</td>\n",
              "      <td>0.452569</td>\n",
              "      <td>0.684492</td>\n",
              "      <td>0.845361</td>\n",
              "      <td>0.293478</td>\n",
              "      <td>0.317241</td>\n",
              "      <td>0.050633</td>\n",
              "      <td>0.943396</td>\n",
              "      <td>0.254355</td>\n",
              "      <td>0.511545</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.168498</td>\n",
              "      <td>0.429387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.327485</td>\n",
              "      <td>0.942688</td>\n",
              "      <td>0.684492</td>\n",
              "      <td>0.742268</td>\n",
              "      <td>0.282609</td>\n",
              "      <td>0.279310</td>\n",
              "      <td>0.054852</td>\n",
              "      <td>0.943396</td>\n",
              "      <td>0.240418</td>\n",
              "      <td>0.289520</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>0.169044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.257310</td>\n",
              "      <td>0.480237</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.381443</td>\n",
              "      <td>0.195652</td>\n",
              "      <td>0.644828</td>\n",
              "      <td>0.559072</td>\n",
              "      <td>0.603774</td>\n",
              "      <td>0.836237</td>\n",
              "      <td>0.049734</td>\n",
              "      <td>0.967033</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.091298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>0.183794</td>\n",
              "      <td>0.684492</td>\n",
              "      <td>0.613402</td>\n",
              "      <td>0.206522</td>\n",
              "      <td>0.558621</td>\n",
              "      <td>0.160338</td>\n",
              "      <td>0.735849</td>\n",
              "      <td>0.655052</td>\n",
              "      <td>0.888988</td>\n",
              "      <td>0.032967</td>\n",
              "      <td>0.186813</td>\n",
              "      <td>0.243937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.821637</td>\n",
              "      <td>0.642292</td>\n",
              "      <td>0.614973</td>\n",
              "      <td>0.134021</td>\n",
              "      <td>0.630435</td>\n",
              "      <td>0.696552</td>\n",
              "      <td>0.569620</td>\n",
              "      <td>0.132075</td>\n",
              "      <td>0.581882</td>\n",
              "      <td>0.298401</td>\n",
              "      <td>0.384615</td>\n",
              "      <td>0.827839</td>\n",
              "      <td>0.343795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.608187</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.690722</td>\n",
              "      <td>0.184783</td>\n",
              "      <td>0.310345</td>\n",
              "      <td>0.316456</td>\n",
              "      <td>0.264151</td>\n",
              "      <td>0.216028</td>\n",
              "      <td>0.177620</td>\n",
              "      <td>0.483516</td>\n",
              "      <td>0.553114</td>\n",
              "      <td>0.138374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.383041</td>\n",
              "      <td>0.199605</td>\n",
              "      <td>0.491979</td>\n",
              "      <td>0.613402</td>\n",
              "      <td>0.152174</td>\n",
              "      <td>0.137931</td>\n",
              "      <td>0.299578</td>\n",
              "      <td>0.660377</td>\n",
              "      <td>0.425087</td>\n",
              "      <td>0.138544</td>\n",
              "      <td>0.373626</td>\n",
              "      <td>0.421245</td>\n",
              "      <td>0.149786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.649123</td>\n",
              "      <td>0.211462</td>\n",
              "      <td>0.716578</td>\n",
              "      <td>0.340206</td>\n",
              "      <td>0.456522</td>\n",
              "      <td>0.644828</td>\n",
              "      <td>0.542194</td>\n",
              "      <td>0.320755</td>\n",
              "      <td>0.365854</td>\n",
              "      <td>0.493783</td>\n",
              "      <td>0.813187</td>\n",
              "      <td>0.589744</td>\n",
              "      <td>0.736091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.257310</td>\n",
              "      <td>0.413043</td>\n",
              "      <td>0.459893</td>\n",
              "      <td>0.381443</td>\n",
              "      <td>0.195652</td>\n",
              "      <td>0.506897</td>\n",
              "      <td>0.402954</td>\n",
              "      <td>0.226415</td>\n",
              "      <td>0.550523</td>\n",
              "      <td>0.036412</td>\n",
              "      <td>0.670330</td>\n",
              "      <td>0.743590</td>\n",
              "      <td>0.008559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.119883</td>\n",
              "      <td>0.193676</td>\n",
              "      <td>0.278075</td>\n",
              "      <td>0.458763</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>0.524138</td>\n",
              "      <td>0.274262</td>\n",
              "      <td>0.452830</td>\n",
              "      <td>0.351916</td>\n",
              "      <td>0.028419</td>\n",
              "      <td>0.439560</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.097718</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>141 rows Ã— 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       1     14.23      1.71      2.43  ...      5.64      1.04      3.92      1065\n",
              "0    1.0  0.304094  0.452569  0.684492  ...  0.511545  0.142857  0.168498  0.429387\n",
              "1    1.0  0.327485  0.942688  0.684492  ...  0.289520  0.307692  0.153846  0.169044\n",
              "2    0.5  0.257310  0.480237  0.454545  ...  0.049734  0.967033  0.571429  0.091298\n",
              "3    1.0  0.605263  0.183794  0.684492  ...  0.888988  0.032967  0.186813  0.243937\n",
              "4    0.0  0.821637  0.642292  0.614973  ...  0.298401  0.384615  0.827839  0.343795\n",
              "..   ...       ...       ...       ...  ...       ...       ...       ...       ...\n",
              "136  0.5  0.608187  0.181818  0.470588  ...  0.177620  0.483516  0.553114  0.138374\n",
              "137  0.5  0.383041  0.199605  0.491979  ...  0.138544  0.373626  0.421245  0.149786\n",
              "138  0.0  0.649123  0.211462  0.716578  ...  0.493783  0.813187  0.589744  0.736091\n",
              "139  0.5  0.257310  0.413043  0.459893  ...  0.036412  0.670330  0.743590  0.008559\n",
              "140  0.5  0.119883  0.193676  0.278075  ...  0.028419  0.439560  0.428571  0.097718\n",
              "\n",
              "[141 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWF3u55rknOE"
      },
      "source": [
        "#Model Building\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xoir8TH3kyb6"
      },
      "source": [
        "# Sequential Layer\n",
        "model = Sequential()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xgMnntrk0IH"
      },
      "source": [
        "# Hidden layers\n",
        "model.add(Dense(units=50, activation='relu'))\n",
        "model.add(Dense(units=100, activation='relu'))\n",
        "model.add(Dense(units=50, activation='relu'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BxgzsNzk4kg"
      },
      "source": [
        "# Output Layer\n",
        "model.add(Dense(1))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2RWi4Qbk8oP"
      },
      "source": [
        "model.compile(loss='mean_squared_error', optimizer='adam')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdLRE4s7lGue"
      },
      "source": [
        "X = scaled_train_df.drop(target, axis=1).values\n",
        "y = scaled_train_df[[target]].values"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyuzR200lINz",
        "outputId": "b425a7cf-f408-441f-ec16-3a818ad99744"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "model.fit(\n",
        "    x=X,\n",
        "    y=y,\n",
        "    epochs=100,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "5/5 - 1s - loss: 0.1934\n",
            "Epoch 2/100\n",
            "5/5 - 0s - loss: 0.0417\n",
            "Epoch 3/100\n",
            "5/5 - 0s - loss: 0.0445\n",
            "Epoch 4/100\n",
            "5/5 - 0s - loss: 0.0376\n",
            "Epoch 5/100\n",
            "5/5 - 0s - loss: 0.0257\n",
            "Epoch 6/100\n",
            "5/5 - 0s - loss: 0.0218\n",
            "Epoch 7/100\n",
            "5/5 - 0s - loss: 0.0193\n",
            "Epoch 8/100\n",
            "5/5 - 0s - loss: 0.0163\n",
            "Epoch 9/100\n",
            "5/5 - 0s - loss: 0.0144\n",
            "Epoch 10/100\n",
            "5/5 - 0s - loss: 0.0130\n",
            "Epoch 11/100\n",
            "5/5 - 0s - loss: 0.0121\n",
            "Epoch 12/100\n",
            "5/5 - 0s - loss: 0.0115\n",
            "Epoch 13/100\n",
            "5/5 - 0s - loss: 0.0114\n",
            "Epoch 14/100\n",
            "5/5 - 0s - loss: 0.0108\n",
            "Epoch 15/100\n",
            "5/5 - 0s - loss: 0.0105\n",
            "Epoch 16/100\n",
            "5/5 - 0s - loss: 0.0102\n",
            "Epoch 17/100\n",
            "5/5 - 0s - loss: 0.0100\n",
            "Epoch 18/100\n",
            "5/5 - 0s - loss: 0.0099\n",
            "Epoch 19/100\n",
            "5/5 - 0s - loss: 0.0096\n",
            "Epoch 20/100\n",
            "5/5 - 0s - loss: 0.0093\n",
            "Epoch 21/100\n",
            "5/5 - 0s - loss: 0.0092\n",
            "Epoch 22/100\n",
            "5/5 - 0s - loss: 0.0090\n",
            "Epoch 23/100\n",
            "5/5 - 0s - loss: 0.0088\n",
            "Epoch 24/100\n",
            "5/5 - 0s - loss: 0.0091\n",
            "Epoch 25/100\n",
            "5/5 - 0s - loss: 0.0084\n",
            "Epoch 26/100\n",
            "5/5 - 0s - loss: 0.0088\n",
            "Epoch 27/100\n",
            "5/5 - 0s - loss: 0.0083\n",
            "Epoch 28/100\n",
            "5/5 - 0s - loss: 0.0084\n",
            "Epoch 29/100\n",
            "5/5 - 0s - loss: 0.0087\n",
            "Epoch 30/100\n",
            "5/5 - 0s - loss: 0.0081\n",
            "Epoch 31/100\n",
            "5/5 - 0s - loss: 0.0083\n",
            "Epoch 32/100\n",
            "5/5 - 0s - loss: 0.0079\n",
            "Epoch 33/100\n",
            "5/5 - 0s - loss: 0.0076\n",
            "Epoch 34/100\n",
            "5/5 - 0s - loss: 0.0075\n",
            "Epoch 35/100\n",
            "5/5 - 0s - loss: 0.0073\n",
            "Epoch 36/100\n",
            "5/5 - 0s - loss: 0.0072\n",
            "Epoch 37/100\n",
            "5/5 - 0s - loss: 0.0072\n",
            "Epoch 38/100\n",
            "5/5 - 0s - loss: 0.0069\n",
            "Epoch 39/100\n",
            "5/5 - 0s - loss: 0.0069\n",
            "Epoch 40/100\n",
            "5/5 - 0s - loss: 0.0069\n",
            "Epoch 41/100\n",
            "5/5 - 0s - loss: 0.0066\n",
            "Epoch 42/100\n",
            "5/5 - 0s - loss: 0.0065\n",
            "Epoch 43/100\n",
            "5/5 - 0s - loss: 0.0064\n",
            "Epoch 44/100\n",
            "5/5 - 0s - loss: 0.0063\n",
            "Epoch 45/100\n",
            "5/5 - 0s - loss: 0.0063\n",
            "Epoch 46/100\n",
            "5/5 - 0s - loss: 0.0065\n",
            "Epoch 47/100\n",
            "5/5 - 0s - loss: 0.0064\n",
            "Epoch 48/100\n",
            "5/5 - 0s - loss: 0.0059\n",
            "Epoch 49/100\n",
            "5/5 - 0s - loss: 0.0061\n",
            "Epoch 50/100\n",
            "5/5 - 0s - loss: 0.0061\n",
            "Epoch 51/100\n",
            "5/5 - 0s - loss: 0.0057\n",
            "Epoch 52/100\n",
            "5/5 - 0s - loss: 0.0057\n",
            "Epoch 53/100\n",
            "5/5 - 0s - loss: 0.0055\n",
            "Epoch 54/100\n",
            "5/5 - 0s - loss: 0.0055\n",
            "Epoch 55/100\n",
            "5/5 - 0s - loss: 0.0053\n",
            "Epoch 56/100\n",
            "5/5 - 0s - loss: 0.0054\n",
            "Epoch 57/100\n",
            "5/5 - 0s - loss: 0.0052\n",
            "Epoch 58/100\n",
            "5/5 - 0s - loss: 0.0051\n",
            "Epoch 59/100\n",
            "5/5 - 0s - loss: 0.0050\n",
            "Epoch 60/100\n",
            "5/5 - 0s - loss: 0.0051\n",
            "Epoch 61/100\n",
            "5/5 - 0s - loss: 0.0047\n",
            "Epoch 62/100\n",
            "5/5 - 0s - loss: 0.0046\n",
            "Epoch 63/100\n",
            "5/5 - 0s - loss: 0.0047\n",
            "Epoch 64/100\n",
            "5/5 - 0s - loss: 0.0046\n",
            "Epoch 65/100\n",
            "5/5 - 0s - loss: 0.0045\n",
            "Epoch 66/100\n",
            "5/5 - 0s - loss: 0.0045\n",
            "Epoch 67/100\n",
            "5/5 - 0s - loss: 0.0044\n",
            "Epoch 68/100\n",
            "5/5 - 0s - loss: 0.0043\n",
            "Epoch 69/100\n",
            "5/5 - 0s - loss: 0.0041\n",
            "Epoch 70/100\n",
            "5/5 - 0s - loss: 0.0045\n",
            "Epoch 71/100\n",
            "5/5 - 0s - loss: 0.0042\n",
            "Epoch 72/100\n",
            "5/5 - 0s - loss: 0.0041\n",
            "Epoch 73/100\n",
            "5/5 - 0s - loss: 0.0047\n",
            "Epoch 74/100\n",
            "5/5 - 0s - loss: 0.0039\n",
            "Epoch 75/100\n",
            "5/5 - 0s - loss: 0.0039\n",
            "Epoch 76/100\n",
            "5/5 - 0s - loss: 0.0039\n",
            "Epoch 77/100\n",
            "5/5 - 0s - loss: 0.0038\n",
            "Epoch 78/100\n",
            "5/5 - 0s - loss: 0.0037\n",
            "Epoch 79/100\n",
            "5/5 - 0s - loss: 0.0035\n",
            "Epoch 80/100\n",
            "5/5 - 0s - loss: 0.0037\n",
            "Epoch 81/100\n",
            "5/5 - 0s - loss: 0.0036\n",
            "Epoch 82/100\n",
            "5/5 - 0s - loss: 0.0035\n",
            "Epoch 83/100\n",
            "5/5 - 0s - loss: 0.0035\n",
            "Epoch 84/100\n",
            "5/5 - 0s - loss: 0.0034\n",
            "Epoch 85/100\n",
            "5/5 - 0s - loss: 0.0034\n",
            "Epoch 86/100\n",
            "5/5 - 0s - loss: 0.0033\n",
            "Epoch 87/100\n",
            "5/5 - 0s - loss: 0.0033\n",
            "Epoch 88/100\n",
            "5/5 - 0s - loss: 0.0034\n",
            "Epoch 89/100\n",
            "5/5 - 0s - loss: 0.0038\n",
            "Epoch 90/100\n",
            "5/5 - 0s - loss: 0.0033\n",
            "Epoch 91/100\n",
            "5/5 - 0s - loss: 0.0032\n",
            "Epoch 92/100\n",
            "5/5 - 0s - loss: 0.0031\n",
            "Epoch 93/100\n",
            "5/5 - 0s - loss: 0.0030\n",
            "Epoch 94/100\n",
            "5/5 - 0s - loss: 0.0029\n",
            "Epoch 95/100\n",
            "5/5 - 0s - loss: 0.0028\n",
            "Epoch 96/100\n",
            "5/5 - 0s - loss: 0.0028\n",
            "Epoch 97/100\n",
            "5/5 - 0s - loss: 0.0032\n",
            "Epoch 98/100\n",
            "5/5 - 0s - loss: 0.0029\n",
            "Epoch 99/100\n",
            "5/5 - 0s - loss: 0.0028\n",
            "Epoch 100/100\n",
            "5/5 - 0s - loss: 0.0026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f92e347cd90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWI-TD1BlMu9"
      },
      "source": [
        "def scaler_inverse_transform(value):\n",
        "    scaled_prediction = value\n",
        "    scaled_prediction -= added_factor\n",
        "    return scaled_prediction / multiply_factor"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs_5JdNjlRHU"
      },
      "source": [
        "multiply_factor = scaler.scale_[13]\n",
        "added_factor = scaler.min_[13]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBdmXYUtlXl0",
        "outputId": "152f6e90-c9de-440c-f7cb-79e33248aa55"
      },
      "source": [
        "scaler_inverse_transform(y[:1][0][0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "880.0000000000001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka8weDqOle--",
        "outputId": "66cf6100-446c-4825-bb94-68dad4f10765"
      },
      "source": [
        "prediction = model.predict(X[:])\n",
        "prediction"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.36567166],\n",
              "       [0.27617824],\n",
              "       [0.06985559],\n",
              "       [0.25941682],\n",
              "       [0.3637963 ],\n",
              "       [0.29251248],\n",
              "       [0.16531943],\n",
              "       [0.31024092],\n",
              "       [0.18557417],\n",
              "       [0.72782964],\n",
              "       [0.2064482 ],\n",
              "       [0.05808902],\n",
              "       [0.31241265],\n",
              "       [0.7163513 ],\n",
              "       [0.16829406],\n",
              "       [0.6647754 ],\n",
              "       [0.5541111 ],\n",
              "       [0.2947137 ],\n",
              "       [0.16483039],\n",
              "       [0.06443333],\n",
              "       [0.11784498],\n",
              "       [0.5268334 ],\n",
              "       [0.1092241 ],\n",
              "       [0.44504654],\n",
              "       [0.09162866],\n",
              "       [0.26474875],\n",
              "       [0.40472978],\n",
              "       [0.3216452 ],\n",
              "       [0.7061602 ],\n",
              "       [0.63542086],\n",
              "       [0.61302274],\n",
              "       [0.36207664],\n",
              "       [0.768111  ],\n",
              "       [0.62866336],\n",
              "       [0.25149384],\n",
              "       [0.2738234 ],\n",
              "       [0.10019037],\n",
              "       [0.6706023 ],\n",
              "       [0.25511938],\n",
              "       [0.08080137],\n",
              "       [0.5340757 ],\n",
              "       [0.26069626],\n",
              "       [0.04664315],\n",
              "       [0.25283122],\n",
              "       [0.58206576],\n",
              "       [0.09104184],\n",
              "       [0.52112037],\n",
              "       [0.5157077 ],\n",
              "       [0.14014834],\n",
              "       [0.60791856],\n",
              "       [0.44453123],\n",
              "       [0.2522974 ],\n",
              "       [0.18285955],\n",
              "       [0.05649934],\n",
              "       [0.15310816],\n",
              "       [0.42256436],\n",
              "       [0.07427881],\n",
              "       [0.23884161],\n",
              "       [0.13900708],\n",
              "       [0.24629983],\n",
              "       [0.27344075],\n",
              "       [0.6316606 ],\n",
              "       [0.475302  ],\n",
              "       [0.21651423],\n",
              "       [0.35275173],\n",
              "       [0.25916132],\n",
              "       [0.39744157],\n",
              "       [0.5467956 ],\n",
              "       [0.08932238],\n",
              "       [0.22396798],\n",
              "       [0.2326409 ],\n",
              "       [0.16023047],\n",
              "       [0.3888393 ],\n",
              "       [0.11233873],\n",
              "       [0.14348307],\n",
              "       [0.20182289],\n",
              "       [0.53935504],\n",
              "       [0.5934835 ],\n",
              "       [0.23706053],\n",
              "       [0.34066322],\n",
              "       [0.21642342],\n",
              "       [0.6203928 ],\n",
              "       [0.5295687 ],\n",
              "       [0.24709934],\n",
              "       [0.34468552],\n",
              "       [0.5397354 ],\n",
              "       [0.54343855],\n",
              "       [0.62821895],\n",
              "       [0.3670807 ],\n",
              "       [0.12541665],\n",
              "       [0.28410774],\n",
              "       [0.08502477],\n",
              "       [0.1518075 ],\n",
              "       [0.20301023],\n",
              "       [0.21643543],\n",
              "       [0.25703406],\n",
              "       [0.19520316],\n",
              "       [0.65482605],\n",
              "       [0.8466262 ],\n",
              "       [0.26501364],\n",
              "       [0.28136855],\n",
              "       [0.2058902 ],\n",
              "       [0.944263  ],\n",
              "       [0.4933667 ],\n",
              "       [0.31784782],\n",
              "       [0.29362658],\n",
              "       [0.25191456],\n",
              "       [0.26127163],\n",
              "       [0.11013092],\n",
              "       [0.6164281 ],\n",
              "       [0.06535259],\n",
              "       [0.70851505],\n",
              "       [0.3762372 ],\n",
              "       [0.16696216],\n",
              "       [0.7217163 ],\n",
              "       [0.21200433],\n",
              "       [0.5013215 ],\n",
              "       [0.684676  ],\n",
              "       [0.2590674 ],\n",
              "       [0.25969547],\n",
              "       [0.53061515],\n",
              "       [0.2916812 ],\n",
              "       [0.30881265],\n",
              "       [0.5674702 ],\n",
              "       [0.74853146],\n",
              "       [0.15385336],\n",
              "       [0.2230791 ],\n",
              "       [0.11974759],\n",
              "       [0.20274186],\n",
              "       [0.1796621 ],\n",
              "       [0.10020991],\n",
              "       [0.23574801],\n",
              "       [0.14493594],\n",
              "       [0.12010705],\n",
              "       [0.26015383],\n",
              "       [0.36436784],\n",
              "       [0.19449699],\n",
              "       [0.13175626],\n",
              "       [0.7916572 ],\n",
              "       [0.0704619 ],\n",
              "       [0.13717975]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEl-a9Qal5ie",
        "outputId": "b951e36a-bc5b-45ce-b022-ae5fd426cf19"
      },
      "source": [
        "for idx in range(141):    \n",
        "    print(f\"Actual Price: {scaler_inverse_transform(y[idx][0])* 1000:.2f}$\")\n",
        "    print(f\"Predicted Price: {scaler_inverse_transform(prediction[idx][0])* 1000:.2f}$\")\n",
        "    print(\"-\"*30)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual Price: 880000.00$\n",
            "Predicted Price: 790671.67$\n",
            "------------------------------\n",
            "Actual Price: 515000.00$\n",
            "Predicted Price: 665201.89$\n",
            "------------------------------\n",
            "Actual Price: 406000.00$\n",
            "Predicted Price: 375937.54$\n",
            "------------------------------\n",
            "Actual Price: 620000.00$\n",
            "Predicted Price: 641702.38$\n",
            "------------------------------\n",
            "Actual Price: 760000.00$\n",
            "Predicted Price: 788042.40$\n",
            "------------------------------\n",
            "Actual Price: 680000.00$\n",
            "Predicted Price: 688102.49$\n",
            "------------------------------\n",
            "Actual Price: 495000.00$\n",
            "Predicted Price: 509777.84$\n",
            "------------------------------\n",
            "Actual Price: 718000.00$\n",
            "Predicted Price: 712957.78$\n",
            "------------------------------\n",
            "Actual Price: 550000.00$\n",
            "Predicted Price: 538174.99$\n",
            "------------------------------\n",
            "Actual Price: 1480000.00$\n",
            "Predicted Price: 1298417.15$\n",
            "------------------------------\n",
            "Actual Price: 562000.00$\n",
            "Predicted Price: 567440.37$\n",
            "------------------------------\n",
            "Actual Price: 380000.00$\n",
            "Predicted Price: 359440.81$\n",
            "------------------------------\n",
            "Actual Price: 640000.00$\n",
            "Predicted Price: 716002.53$\n",
            "------------------------------\n",
            "Actual Price: 1190000.00$\n",
            "Predicted Price: 1282324.48$\n",
            "------------------------------\n",
            "Actual Price: 500000.00$\n",
            "Predicted Price: 513948.27$\n",
            "------------------------------\n",
            "Actual Price: 1270000.00$\n",
            "Predicted Price: 1210015.07$\n",
            "------------------------------\n",
            "Actual Price: 1285000.00$\n",
            "Predicted Price: 1054863.79$\n",
            "------------------------------\n",
            "Actual Price: 675000.00$\n",
            "Predicted Price: 691188.62$\n",
            "------------------------------\n",
            "Actual Price: 580000.00$\n",
            "Predicted Price: 509092.20$\n",
            "------------------------------\n",
            "Actual Price: 315000.00$\n",
            "Predicted Price: 368335.53$\n",
            "------------------------------\n",
            "Actual Price: 463000.00$\n",
            "Predicted Price: 443218.67$\n",
            "------------------------------\n",
            "Actual Price: 1105000.00$\n",
            "Predicted Price: 1016620.45$\n",
            "------------------------------\n",
            "Actual Price: 345000.00$\n",
            "Predicted Price: 431132.19$\n",
            "------------------------------\n",
            "Actual Price: 1015000.00$\n",
            "Predicted Price: 901955.25$\n",
            "------------------------------\n",
            "Actual Price: 415000.00$\n",
            "Predicted Price: 406463.38$\n",
            "------------------------------\n",
            "Actual Price: 660000.00$\n",
            "Predicted Price: 649177.75$\n",
            "------------------------------\n",
            "Actual Price: 870000.00$\n",
            "Predicted Price: 845431.16$\n",
            "------------------------------\n",
            "Actual Price: 685000.00$\n",
            "Predicted Price: 728946.57$\n",
            "------------------------------\n",
            "Actual Price: 1320000.00$\n",
            "Predicted Price: 1268036.58$\n",
            "------------------------------\n",
            "Actual Price: 1295000.00$\n",
            "Predicted Price: 1168860.04$\n",
            "------------------------------\n",
            "Actual Price: 915000.00$\n",
            "Predicted Price: 1137457.89$\n",
            "------------------------------\n",
            "Actual Price: 855000.00$\n",
            "Predicted Price: 785631.45$\n",
            "------------------------------\n",
            "Actual Price: 1450000.00$\n",
            "Predicted Price: 1354891.61$\n",
            "------------------------------\n",
            "Actual Price: 1235000.00$\n",
            "Predicted Price: 1159386.03$\n",
            "------------------------------\n",
            "Actual Price: 625000.00$\n",
            "Predicted Price: 630594.37$\n",
            "------------------------------\n",
            "Actual Price: 510000.00$\n",
            "Predicted Price: 661900.42$\n",
            "------------------------------\n",
            "Actual Price: 392000.00$\n",
            "Predicted Price: 418466.90$\n",
            "------------------------------\n",
            "Actual Price: 1280000.00$\n",
            "Predicted Price: 1218184.45$\n",
            "------------------------------\n",
            "Actual Price: 625000.00$\n",
            "Predicted Price: 635677.38$\n",
            "------------------------------\n",
            "Actual Price: 380000.00$\n",
            "Predicted Price: 391283.52$\n",
            "------------------------------\n",
            "Actual Price: 1050000.00$\n",
            "Predicted Price: 1026774.10$\n",
            "------------------------------\n",
            "Actual Price: 650000.00$\n",
            "Predicted Price: 643496.16$\n",
            "------------------------------\n",
            "Actual Price: 352000.00$\n",
            "Predicted Price: 343393.70$\n",
            "------------------------------\n",
            "Actual Price: 678000.00$\n",
            "Predicted Price: 632469.37$\n",
            "------------------------------\n",
            "Actual Price: 1080000.00$\n",
            "Predicted Price: 1094056.20$\n",
            "------------------------------\n",
            "Actual Price: 428000.00$\n",
            "Predicted Price: 405640.66$\n",
            "------------------------------\n",
            "Actual Price: 1035000.00$\n",
            "Predicted Price: 1008610.76$\n",
            "------------------------------\n",
            "Actual Price: 1035000.00$\n",
            "Predicted Price: 1001022.16$\n",
            "------------------------------\n",
            "Actual Price: 466000.00$\n",
            "Predicted Price: 474487.98$\n",
            "------------------------------\n",
            "Actual Price: 1195000.00$\n",
            "Predicted Price: 1130301.82$\n",
            "------------------------------\n",
            "Actual Price: 845000.00$\n",
            "Predicted Price: 901232.79$\n",
            "------------------------------\n",
            "Actual Price: 580000.00$\n",
            "Predicted Price: 631720.96$\n",
            "------------------------------\n",
            "Actual Price: 630000.00$\n",
            "Predicted Price: 534369.10$\n",
            "------------------------------\n",
            "Actual Price: 278000.00$\n",
            "Predicted Price: 357212.07$\n",
            "------------------------------\n",
            "Actual Price: 450000.00$\n",
            "Predicted Price: 492657.65$\n",
            "------------------------------\n",
            "Actual Price: 795000.00$\n",
            "Predicted Price: 870435.23$\n",
            "------------------------------\n",
            "Actual Price: 438000.00$\n",
            "Predicted Price: 382138.89$\n",
            "------------------------------\n",
            "Actual Price: 562000.00$\n",
            "Predicted Price: 612855.93$\n",
            "------------------------------\n",
            "Actual Price: 465000.00$\n",
            "Predicted Price: 472887.92$\n",
            "------------------------------\n",
            "Actual Price: 740000.00$\n",
            "Predicted Price: 623312.37$\n",
            "------------------------------\n",
            "Actual Price: 550000.00$\n",
            "Predicted Price: 661363.93$\n",
            "------------------------------\n",
            "Actual Price: 1060000.00$\n",
            "Predicted Price: 1163588.13$\n",
            "------------------------------\n",
            "Actual Price: 937000.00$\n",
            "Predicted Price: 944373.42$\n",
            "------------------------------\n",
            "Actual Price: 520000.00$\n",
            "Predicted Price: 581552.95$\n",
            "------------------------------\n",
            "Actual Price: 695000.00$\n",
            "Predicted Price: 772557.93$\n",
            "------------------------------\n",
            "Actual Price: 630000.00$\n",
            "Predicted Price: 641344.18$\n",
            "------------------------------\n",
            "Actual Price: 886000.00$\n",
            "Predicted Price: 835213.08$\n",
            "------------------------------\n",
            "Actual Price: 1095000.00$\n",
            "Predicted Price: 1044607.44$\n",
            "------------------------------\n",
            "Actual Price: 378000.00$\n",
            "Predicted Price: 403229.98$\n",
            "------------------------------\n",
            "Actual Price: 600000.00$\n",
            "Predicted Price: 592003.11$\n",
            "------------------------------\n",
            "Actual Price: 590000.00$\n",
            "Predicted Price: 604162.55$\n",
            "------------------------------\n",
            "Actual Price: 420000.00$\n",
            "Predicted Price: 502643.12$\n",
            "------------------------------\n",
            "Actual Price: 840000.00$\n",
            "Predicted Price: 823152.70$\n",
            "------------------------------\n",
            "Actual Price: 434000.00$\n",
            "Predicted Price: 435498.90$\n",
            "------------------------------\n",
            "Actual Price: 495000.00$\n",
            "Predicted Price: 479163.27$\n",
            "------------------------------\n",
            "Actual Price: 510000.00$\n",
            "Predicted Price: 560955.69$\n",
            "------------------------------\n",
            "Actual Price: 1060000.00$\n",
            "Predicted Price: 1034175.77$\n",
            "------------------------------\n",
            "Actual Price: 1290000.00$\n",
            "Predicted Price: 1110063.88$\n",
            "------------------------------\n",
            "Actual Price: 520000.00$\n",
            "Predicted Price: 610358.87$\n",
            "------------------------------\n",
            "Actual Price: 735000.00$\n",
            "Predicted Price: 755609.84$\n",
            "------------------------------\n",
            "Actual Price: 520000.00$\n",
            "Predicted Price: 581425.64$\n",
            "------------------------------\n",
            "Actual Price: 1150000.00$\n",
            "Predicted Price: 1147790.70$\n",
            "------------------------------\n",
            "Actual Price: 880000.00$\n",
            "Predicted Price: 1020455.28$\n",
            "------------------------------\n",
            "Actual Price: 714000.00$\n",
            "Predicted Price: 624433.27$\n",
            "------------------------------\n",
            "Actual Price: 750000.00$\n",
            "Predicted Price: 761249.11$\n",
            "------------------------------\n",
            "Actual Price: 920000.00$\n",
            "Predicted Price: 1034709.00$\n",
            "------------------------------\n",
            "Actual Price: 985000.00$\n",
            "Predicted Price: 1039900.85$\n",
            "------------------------------\n",
            "Actual Price: 1045000.00$\n",
            "Predicted Price: 1158762.97$\n",
            "------------------------------\n",
            "Actual Price: 885000.00$\n",
            "Predicted Price: 792647.13$\n",
            "------------------------------\n",
            "Actual Price: 410000.00$\n",
            "Predicted Price: 453834.15$\n",
            "------------------------------\n",
            "Actual Price: 720000.00$\n",
            "Predicted Price: 676319.06$\n",
            "------------------------------\n",
            "Actual Price: 325000.00$\n",
            "Predicted Price: 397204.73$\n",
            "------------------------------\n",
            "Actual Price: 495000.00$\n",
            "Predicted Price: 490834.12$\n",
            "------------------------------\n",
            "Actual Price: 520000.00$\n",
            "Predicted Price: 562620.34$\n",
            "------------------------------\n",
            "Actual Price: 480000.00$\n",
            "Predicted Price: 581442.48$\n",
            "------------------------------\n",
            "Actual Price: 680000.00$\n",
            "Predicted Price: 638361.76$\n",
            "------------------------------\n",
            "Actual Price: 480000.00$\n",
            "Predicted Price: 551674.82$\n",
            "------------------------------\n",
            "Actual Price: 1045000.00$\n",
            "Predicted Price: 1196066.12$\n",
            "------------------------------\n",
            "Actual Price: 1547000.00$\n",
            "Predicted Price: 1464969.96$\n",
            "------------------------------\n",
            "Actual Price: 680000.00$\n",
            "Predicted Price: 649549.12$\n",
            "------------------------------\n",
            "Actual Price: 615000.00$\n",
            "Predicted Price: 672478.71$\n",
            "------------------------------\n",
            "Actual Price: 560000.00$\n",
            "Predicted Price: 566658.05$\n",
            "------------------------------\n",
            "Actual Price: 1680000.00$\n",
            "Predicted Price: 1601856.70$\n",
            "------------------------------\n",
            "Actual Price: 985000.00$\n",
            "Predicted Price: 969700.10$\n",
            "------------------------------\n",
            "Actual Price: 725000.00$\n",
            "Predicted Price: 723622.64$\n",
            "------------------------------\n",
            "Actual Price: 695000.00$\n",
            "Predicted Price: 689664.46$\n",
            "------------------------------\n",
            "Actual Price: 650000.00$\n",
            "Predicted Price: 631184.21$\n",
            "------------------------------\n",
            "Actual Price: 675000.00$\n",
            "Predicted Price: 644302.82$\n",
            "------------------------------\n",
            "Actual Price: 355000.00$\n",
            "Predicted Price: 432403.55$\n",
            "------------------------------\n",
            "Actual Price: 1120000.00$\n",
            "Predicted Price: 1142232.16$\n",
            "------------------------------\n",
            "Actual Price: 312000.00$\n",
            "Predicted Price: 369624.33$\n",
            "------------------------------\n",
            "Actual Price: 1265000.00$\n",
            "Predicted Price: 1271338.10$\n",
            "------------------------------\n",
            "Actual Price: 835000.00$\n",
            "Predicted Price: 805484.57$\n",
            "------------------------------\n",
            "Actual Price: 520000.00$\n",
            "Predicted Price: 512080.95$\n",
            "------------------------------\n",
            "Actual Price: 1260000.00$\n",
            "Predicted Price: 1289846.23$\n",
            "------------------------------\n",
            "Actual Price: 625000.00$\n",
            "Predicted Price: 575230.08$\n",
            "------------------------------\n",
            "Actual Price: 1035000.00$\n",
            "Predicted Price: 980852.74$\n",
            "------------------------------\n",
            "Actual Price: 1285000.00$\n",
            "Predicted Price: 1237915.74$\n",
            "------------------------------\n",
            "Actual Price: 660000.00$\n",
            "Predicted Price: 641212.48$\n",
            "------------------------------\n",
            "Actual Price: 630000.00$\n",
            "Predicted Price: 642093.05$\n",
            "------------------------------\n",
            "Actual Price: 1020000.00$\n",
            "Predicted Price: 1021922.44$\n",
            "------------------------------\n",
            "Actual Price: 660000.00$\n",
            "Predicted Price: 686937.04$\n",
            "------------------------------\n",
            "Actual Price: 750000.00$\n",
            "Predicted Price: 710955.33$\n",
            "------------------------------\n",
            "Actual Price: 1185000.00$\n",
            "Predicted Price: 1073593.21$\n",
            "------------------------------\n",
            "Actual Price: 1375000.00$\n",
            "Predicted Price: 1327441.11$\n",
            "------------------------------\n",
            "Actual Price: 500000.00$\n",
            "Predicted Price: 493702.41$\n",
            "------------------------------\n",
            "Actual Price: 530000.00$\n",
            "Predicted Price: 590756.90$\n",
            "------------------------------\n",
            "Actual Price: 425000.00$\n",
            "Predicted Price: 445886.12$\n",
            "------------------------------\n",
            "Actual Price: 672000.00$\n",
            "Predicted Price: 562244.09$\n",
            "------------------------------\n",
            "Actual Price: 710000.00$\n",
            "Predicted Price: 529886.26$\n",
            "------------------------------\n",
            "Actual Price: 345000.00$\n",
            "Predicted Price: 418494.30$\n",
            "------------------------------\n",
            "Actual Price: 680000.00$\n",
            "Predicted Price: 608518.71$\n",
            "------------------------------\n",
            "Actual Price: 428000.00$\n",
            "Predicted Price: 481200.18$\n",
            "------------------------------\n",
            "Actual Price: 365000.00$\n",
            "Predicted Price: 446390.08$\n",
            "------------------------------\n",
            "Actual Price: 560000.00$\n",
            "Predicted Price: 642735.67$\n",
            "------------------------------\n",
            "Actual Price: 770000.00$\n",
            "Predicted Price: 788843.72$\n",
            "------------------------------\n",
            "Actual Price: 472000.00$\n",
            "Predicted Price: 550684.78$\n",
            "------------------------------\n",
            "Actual Price: 488000.00$\n",
            "Predicted Price: 462722.28$\n",
            "------------------------------\n",
            "Actual Price: 1310000.00$\n",
            "Predicted Price: 1387903.41$\n",
            "------------------------------\n",
            "Actual Price: 290000.00$\n",
            "Predicted Price: 376787.58$\n",
            "------------------------------\n",
            "Actual Price: 415000.00$\n",
            "Predicted Price: 470326.01$\n",
            "------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1MnbMzXnFnD"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}